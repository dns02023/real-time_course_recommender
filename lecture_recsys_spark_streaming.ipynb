{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.56.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://DESKTOP-K9GD4CQ.localdomain:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://DESKTOP-K9GD4CQ.localdomain:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.56.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://DESKTOP-K9GD4CQ.localdomain:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc3988990f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 행동 로그 데이터 메시지 consume\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "  .option(\"subscribe\", \"lecture-recsys-log\") \\\n",
    "  .option(\"failOnDataLoss\", \"false\") \\\n",
    "  .load()\n",
    "ds = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# 추천 결과 데이터 메시지 consume\n",
    "df2 = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "  .option(\"subscribe\", \"lecture-recommenders\") \\\n",
    "  .option(\"failOnDataLoss\", \"false\") \\\n",
    "  .load()\n",
    "ds2 = df2.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.printSchema()\n",
    "ds2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType, LongType\n",
    "schema = StructType().add(\"user_id\", IntegerType()).add(\"action\", StringType()).add(\"lecture_id\", IntegerType())\n",
    "#schema2 = StructType().add(\"user_id\", IntegerType()).add(\"count_recommenders\", MapType(StringType(), LongType()))\n",
    "schema2 = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"best\", ArrayType(IntegerType()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- jsontostructs(value): struct (nullable = true)\n",
      " |    |-- user_id: integer (nullable = true)\n",
      " |    |-- action: string (nullable = true)\n",
      " |    |-- lecture_id: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- jsontostructs(value): struct (nullable = true)\n",
      " |    |-- user_id: integer (nullable = true)\n",
      " |    |-- best: array (nullable = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = ds.select(from_json(\"value\", schema)).alias(\"parsed\")\n",
    "data2 = ds2.select(from_json(\"value\", schema2)).alias(\"parsed2\")\n",
    "data.printSchema()\n",
    "data2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- lecture_id: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- best: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logdf = data.select(\"jsontostructs(value).*\")\n",
    "logdf2 = data2.select(\"jsontostructs(value).*\")\n",
    "logdf.printSchema()\n",
    "logdf2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with open('cosine_sim_final.pkl','rb') as f:\n",
    "    cosine_sim=pickle.load(f)\n",
    "    \n",
    "x = pd.Series(cosine_sim[1]).index+1\n",
    "# 유사 강의 목록 구하기 + 점수 부여\n",
    "def get_recommendations(i, action):\n",
    "    sim_scores = list(enumerate(cosine_sim[i-1]))\n",
    "    sim_scores = sorted(sim_scores, key = lambda x : x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:6] \n",
    "    recommend = list(x[[s[0] for s in sim_scores]])\n",
    "    \n",
    "    if action == 'CLICK':\n",
    "        for i in range(len(recommend)):\n",
    "            recommend[i] = [recommend[i], 1]\n",
    "    elif action == 'WISH':\n",
    "        for i in range(len(recommend)):\n",
    "            recommend[i] = [recommend[i], 3]\n",
    "        \n",
    "    return recommend\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_udf = F.udf(lambda lecture_id, action: get_recommendations(lecture_id, action), ArrayType(ArrayType(IntegerType())))\n",
    "recsys_df = logdf.withColumn('recommenders', recommender_udf(logdf.lecture_id, logdf.action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- lecture_id: integer (nullable = true)\n",
      " |-- recommenders: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recsys_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- collect_list(recommenders): array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- recommenders: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "def reduce_f(val):\n",
    "    return functools.reduce(lambda x,y:x+y,val)\n",
    "\n",
    "flatten_udf = F.udf(reduce_f, ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "temp_df = recsys_df.groupBy(\"user_id\").agg(F.collect_list(\"recommenders\"))\n",
    "temp_df.printSchema()\n",
    "\n",
    "group_recsys_df = temp_df.select(\"user_id\", flatten_udf(\"collect_list(recommenders)\").alias(\"recommenders\"))\n",
    "group_recsys_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5개의 상위 점수 강의들로 추천 list 구성\n",
    "def recsys_count(recommenders):\n",
    "    temp = dict()\n",
    "    for rec in recommenders:\n",
    "        if rec[0] in temp:\n",
    "            temp[rec[0]] = temp[rec[0]] + rec[1]\n",
    "        else:\n",
    "            temp[rec[0]] = rec[1]\n",
    "    \n",
    "    rec_list = list()\n",
    "    for _ in range(5):\n",
    "        max_key = max(temp.keys(), key=lambda k: temp[k])\n",
    "        rec_list.append(max_key)\n",
    "        temp.pop(max_key)\n",
    "        \n",
    "    return rec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- recommenders: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- best: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_udf = F.udf(lambda recommenders: recsys_count(recommenders), ArrayType(IntegerType()))\n",
    "final_recsys_df = group_recsys_df.withColumn('best', count_udf(group_recsys_df.recommenders))\n",
    "final_recsys_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- best: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_recsys_df.createOrReplaceTempView('logtable')\n",
    "final_recsys_df_sql = spark.sql(\"select (user_id, best) as value from logtable\").select(to_json(\"value\").alias(\"value\"))\n",
    "logdf2.printSchema()\n",
    "\n",
    "#recsys_df_sql = spark.sql(\"select (user_id, action, lecture_id, recommenders) as value from logtable\").select(to_json(\"value\").alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개인화 추천 결과 produce\n",
    "query1 = final_recsys_df_sql \\\n",
    "        .writeStream \\\n",
    "        .trigger(processingTime='30 seconds') \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "        .option(\"checkpointLocation\",\"pyspark/streaming/checkpointLocation4\") \\\n",
    "        .option(\"topic\", \"lecture-recommenders\") \\\n",
    "        .option(\"failOnDataLoss\", \"false\") \\\n",
    "        .start()\n",
    "# 개인화 추천 결과 Elasticsearch에 적재\n",
    "query2 = logdf2 \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"org.elasticsearch.spark.sql\") \\\n",
    "        .option(\"checkpointLocation\", \"pyspark/streaming/checkpointLocation5\") \\\n",
    "        .option(\"es.resource\", \"recommenders_db\") \\\n",
    "        .option(\"es.nodes\", ELASTICSEARCH_DB) \\\n",
    "        .option(\"failOnDataLoss\", \"false\") \\\n",
    "        .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query1.awaitTermination()\n",
    "query2.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}